{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1> Wprowadzenie do transformacji PCA </h1>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wygenerować dane (1000 punktów) na płaszczyźnie z 2-wymiarowego rozkładu normalnego o niediagonalnej macierzy kowariancji i narysować je."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytamy się, które współrzędne są najważniejsze - opisują najwięcej informacji o naszych danych. Do tego celu możnaby użyć poznanych metod selekcji atrybutów (na którymś z wcześniejszych zajęć).\n",
    "\n",
    "Widać, że dane sa skorelowane - wzrost pierwszego atrybuty pociąga za sobą wzrost drugiego. Dlatego rozsądniej mówić jest o kierunku który jest najważniejszy dla opisu naszych danych. Do tego celu używa się metody analizy głównych składowych (principal component analysis PCA).\n",
    "\n",
    "Pierwszym krokiem jest normalizacja danych. Jako, że atrybut 1 może mieć inną skalę niż atrybut 2, konieczne jest ujednolicenie skal. Dokunujemy tego w dwóch krokach:\n",
    "\n",
    "-przesuwamy dane do środka układu współrzędnych\n",
    "\n",
    "-normalizujemy dane (dzielimy każdą współrzędną przez średnią długość wszyskich punktów)\n",
    "\n",
    "Możemy to zrobić używając scalera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wygląd naszych danych jest niemal identyczny jak poprzednio. Jedyna zmiana polega na zmianie skali. Tak naprawdę odchylenie standardowe na każdym atrybucie wynosi 1. \n",
    "\n",
    "\n",
    "Proszę sprawdzić, jakie jest odchylenie standardowe na każdej współrzędnej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Aby wyznaczyć kierunki decydujące o kształcie naszych danych, należy policzyć wektory i wartości wałasne z macierzy kowariancji.\n",
    "\n",
    "Definicja: Dla macierzy kwadratowej $A$, wektor własny $v$ i wartość własna $\\lambda$ spełnia: \n",
    "$$\n",
    "Av = \\lambda v\n",
    "$$\n",
    "\n",
    "Zilustrujemy powyższe wielkości na przykładzie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narysujmy oba wektory na naszych danych przeskalowane przez odpowiadające im wartości własne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co zrobiliśmy (trochę matematyki)? \n",
    "\n",
    "Przedstawiliśmy macierz kowariancji $\\Sigma$ w postaci iloczynu:\n",
    "$$\n",
    "\\Sigma = V S V^T\n",
    "$$\n",
    "gdzie $V$ to macierz zawierająca na kolumnach wektory własne, a $S$ to macierz diagonalna, która na przekątnej ma wartości własne.\n",
    "\n",
    "Zweryfikować powyższe stwierdzenie tzn. sprawdzić, że powyższy iloczyn rzeczywiście daje macierz kowariancji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyznaczanie wektorów i wartości własnych macierzy kowariancji danych, to podstawa PCA. Metoda PCA wykorzystuje znalezone kierunki (wektory własne) do dekorelacji danych (obrotu wokół układu współrzędnych)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy już zmniejszyć wymiar 4-wymiarowych danych do 2-wymiarowe dane. W tym celu wybieramy dwa wektory własne odpowiadające dwóm największym wartością własnym.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy przedstawić nasze dane za pomocą kierunków głównych (wektorów głównych). Intuicyjnie jest to po prostu obrót danych, tak aby wektory własne pokrywały się z głównymi osiami układu współrzędnych.\n",
    "\n",
    "Redukcja wymiarowości to rzutowanie danych na podprzestrzeń wyznaczoną przez wybrane wektory własne i jest to po prostu mnożenie macierzy:\n",
    "$$\n",
    "X V_k\n",
    "$$\n",
    "gdzie $V_k=(v_1,\\ldots,v_k)$ to macierz gdzie w kolumnach są wektory własne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "#proszę zrzutować i narysować"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proszę wykonać PCA używając sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA as pca\n",
    "\n",
    "#TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak otrzymać wektory i wartości własne? Kryją się pod atrybutami components\\_ i explained\\_variance\\_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "#proszę zobaczyć"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy przedstawić nasze dane za pomocą kierunków głównych (wektorów głównych). Intuicyjnie jest to po prostu obrót danych, tak aby wektory własne pokrywały się z głównymi osiami układu współrzędnych.\n",
    "\n",
    "Redukcja wymiarowości to rzutowanie danych na podprzestrzeń wyznaczoną przez wybrane wektory własne i jest to po prostu mnożenie macierzy:\n",
    "$$\n",
    "X V_k\n",
    "$$\n",
    "gdzie $V_k=(v_1,\\ldots,v_k)$ to macierz gdzie w kolumnach są wektory własne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "#po transformacji wynik powinnien być taki sam jak poprzednio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W konsekwencji nasze dane są zdekorelowane - obróciliśmy dane tak że atrybuty nie są między soba zależne (liniowo).\n",
    "\n",
    "Poza zmianą atrybutów (układu współrzędnych) możemy również ograniczyć się do najważniejszych atrybutów, np. tylko do pierwszej współrzędnej. Wówczas rzutujemy dane na atrybut 1. W sklearn robimy to ustawiając paraemtr n\\_components na 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zobaczmy na histogram zredukowanych danych - w dalszym ciągu mają kształt gaussowski."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ZADANIE: Proszę sprawdzić czy dane przed PCA były niezależne oraz czy po PCA stały się niezależne - prosze przeprowadzić odpowiedni test statystyczny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZADANIE: Proszę pobrać dane dwóch akcji z https://archive.ics.uci.edu/ml/datasets/ISTANBUL+STOCK+EXCHANGE. Liczymy macierz kowariancji tych danych. Sprawdzamy testem czy jest w nich zależność. Jeśli tak to próbujemy dekorekować PCA i pytamy się czy zależność zniknęła. Rysujemy na płaszczyźnie w celu ilustracji.\n",
    "\n",
    "Co nam daje PCA tu? Wyznaczamy kierunek największej zmiany np. $a S_A + b S_B$, gdzie $S_A,S_B$ to akcje - to jest po prostu pierwszy wektor własny. Zatem jeśli oczekujemy dużych zysków (ale też strat) to powinniśmy inwestować $a$ w akcję $A$ i $b$ w akcję $B$.\n",
    "\n",
    "Proszę sprawdzić zależność (przed i po dekorelacji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dane mogą być zdekorelowane ale nadal zależne. Proszę wygenerować półksiężyć i zdekorelować - zobaczyć że zależność nadal istnieje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZADANIE: Proszę sprawdzić testem statystycznym czy dane są zależne (przed i po dekorelacji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W praktycem jeśli dane nie są zdekorelowane, to metody uczenia mają tendencję do większych błędów niż gdyby dane były zdekorelowane.  \n",
    "\n",
    "Mimo dokonania dekorelacji, nowe atrybuty mają różne skale. Jest to problem dla metod bazujących na odległościd - odległość na atrybucie o mniejszej skali jest zawsze mniejsza niż na tym o większej. \n",
    "\n",
    "ZADANIE: Proszę spróbować znaleźć najbliższych sąsiadów dla punktu przed skalowaniem danych i porównać ich dla tego samego punktu po skalowaniu.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dlatego po PCA często skaluje się nowe atrybuty (dzieli przez pierwiastki wartości własnych). Robimy z danych kulkę:) Proszę tak zrobić na przykładzie naszych danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Można to samo uzyskać ustawiając prametr whiten na true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
